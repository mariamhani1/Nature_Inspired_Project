# -*- coding: utf-8 -*-
"""Copy of DSAI 403 Full Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2UZ6LkjioKBy_o1BRag6Rgu1QjAxsIU
"""

# Install required packages
!pip install datasets transformers torch scikit-learn numpy pandas matplotlib seaborn

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from collections import Counter
import random
import copy
import time
import warnings
import math
import os
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

# Handle CUDA initialization more carefully
if torch.cuda.is_available():
    try:
        # Clear any existing CUDA cache
        torch.cuda.empty_cache()

        # Set the seed
        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)

        # Set CUDA settings for reproducibility
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        device = torch.device('cuda')
        print(f"Using device: CUDA ({torch.cuda.get_device_name(0)})")
        print(f"CUDA Version: {torch.version.cuda}")
        print(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    except Exception as e:
        print(f"CUDA initialization failed: {e}")
        print("Falling back to CPU")
        device = torch.device('cpu')
        torch.manual_seed(42)
else:
    device = torch.device('cpu')
    torch.manual_seed(42)
    print("CUDA not available. Using CPU")

print(f"Final device: {device}")

# Load and explore the dataset (FIXED VERSION)
dataset = load_dataset('sh0416/ag_news')

# Display dataset information
print("\nDataset structure:")
print(dataset)

print("\nTraining set size:", len(dataset['train']))
print("\nSample from training set:")
sample = dataset['train'][0]
print(sample)

# Check class distribution
train_labels = [item['label'] for item in dataset['train']]
label_counts = Counter(train_labels)

# FIXED: Correct class names (AG News labels are 1-4, not 0-3)
class_names = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}

print("\nClass distribution:")
for label, count in sorted(label_counts.items()):
    print(f"  Label {label} ({class_names.get(label, 'Unknown')}): {count} samples")

print("\nNote: Labels will be converted to 0-indexed (0-3) for PyTorch training")

# Text preprocessing and vocabulary building
from collections import Counter

class TextPreprocessor:
    def __init__(self, max_vocab_size=10000, max_length=200):
        self.max_vocab_size = max_vocab_size
        self.max_length = max_length
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}

    def build_vocab(self, texts):
        """Build vocabulary from texts"""
        word_freq = Counter()
        for text in texts:
            words = text.lower().split()
            word_freq.update(words)

        # Get most common words
        most_common = word_freq.most_common(self.max_vocab_size - 2)

        for idx, (word, freq) in enumerate(most_common, start=2):
            self.word2idx[word] = idx
            self.idx2word[idx] = word

        print(f"Vocabulary size: {len(self.word2idx)}")

    def encode(self, texts):
        """Convert texts to sequences of indices"""
        sequences = []
        for text in texts:
            words = text.lower().split()
            seq = [self.word2idx.get(word, 1) for word in words]

            # Pad or truncate
            if len(seq) < self.max_length:
                seq = seq + [0] * (self.max_length - len(seq))
            else:
                seq = seq[:self.max_length]

            sequences.append(seq)

        return np.array(sequences)

# Prepare the data (FIXED VERSION)
print("Preparing data...")

# Use a subset for faster training (you can adjust this)
train_size = 20000  # Using 20k for training
test_size = 2000    # Using 2k for testing

# Sample data
train_indices = random.sample(range(len(dataset['train'])), train_size)
train_data = dataset['train'].select(train_indices)

# Prepare texts and labels
train_texts = [item['title'] + ' ' + item['description'] for item in train_data]
train_labels = [item['label'] - 1 for item in train_data]  # FIXED: Convert to 0-indexed (subtract 1)

# Split into train and validation
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels
)

print(f"Training samples: {len(train_texts)}")
print(f"Validation samples: {len(val_texts)}")

# Verify label range
print(f"Label range: {min(train_labels)} to {max(train_labels)}")
print(f"Unique labels: {sorted(set(train_labels))}")

# Build vocabulary and encode texts
preprocessor = TextPreprocessor(max_vocab_size=10000, max_length=200)
preprocessor.build_vocab(train_texts)

X_train = preprocessor.encode(train_texts)
X_val = preprocessor.encode(val_texts)
y_train = np.array(train_labels)
y_val = np.array(val_labels)

print(f"X_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"y_train range: {y_train.min()} to {y_train.max()}")

# Define Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: (batch_size, seq_length, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

# Define the Transformer model architecture
class TextTransformer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers,
                 dim_feedforward, num_classes, max_length=200, dropout=0.1):
        super(TextTransformer, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.pos_encoder = PositionalEncoding(embedding_dim, max_length, dropout)

        # Transformer encoder layer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )

        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(embedding_dim, num_classes)

        self.embedding_dim = embedding_dim

    def forward(self, x):
        # x shape: (batch_size, seq_length)

        # Create padding mask
        padding_mask = (x == 0)  # True for padding tokens

        # Embedding
        embedded = self.embedding(x) * math.sqrt(self.embedding_dim)  # (batch_size, seq_length, embedding_dim)
        embedded = self.pos_encoder(embedded)

        # Transformer encoding
        transformer_out = self.transformer_encoder(
            embedded,
            src_key_padding_mask=padding_mask
        )  # (batch_size, seq_length, embedding_dim)

        # Global average pooling over sequence dimension
        # Mask out padding tokens before pooling
        mask_expanded = (~padding_mask).unsqueeze(-1).float()  # (batch_size, seq_length, 1)
        summed = (transformer_out * mask_expanded).sum(dim=1)  # (batch_size, embedding_dim)
        counts = mask_expanded.sum(dim=1)  # (batch_size, 1)
        pooled = summed / (counts + 1e-9)  # (batch_size, embedding_dim)

        # Classification
        pooled = self.dropout(pooled)
        output = self.fc(pooled)  # (batch_size, num_classes)

        return output

# Training and evaluation functions
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, verbose=True):
    """Train the model and return training history"""
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

        train_loss /= len(train_loader)
        train_acc = 100 * train_correct / train_total

        # Validation phase
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_loss /= len(val_loader)
        val_acc = 100 * val_correct / val_total

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        if verbose and (epoch + 1) % 2 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

    return history

def evaluate_model(model, data_loader):
    """Evaluate model and return accuracy"""
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    return accuracy

# Train baseline model

# Hyperparameters for baseline
baseline_params = {
    'embedding_dim': 128,
    'num_heads': 4,
    'num_layers': 2,
    'dim_feedforward': 512,
    'dropout': 0.1,
    'learning_rate': 0.001,
    'batch_size': 64,
    'epochs': 10
}

print("Baseline Hyperparameters:")
for key, value in baseline_params.items():
    print(f"  {key}: {value}")

# Create data loaders
train_dataset = TensorDataset(
    torch.LongTensor(X_train),
    torch.LongTensor(y_train)
)
val_dataset = TensorDataset(
    torch.LongTensor(X_val),
    torch.LongTensor(y_val)
)

train_loader = DataLoader(train_dataset, batch_size=baseline_params['batch_size'], shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=baseline_params['batch_size'])

# Initialize baseline model
baseline_model = TextTransformer(
    vocab_size=len(preprocessor.word2idx),
    embedding_dim=baseline_params['embedding_dim'],
    num_heads=baseline_params['num_heads'],
    num_layers=baseline_params['num_layers'],
    dim_feedforward=baseline_params['dim_feedforward'],
    num_classes=4,
    max_length=preprocessor.max_length,
    dropout=baseline_params['dropout']
).to(device)

print(f"\nModel Parameters: {sum(p.numel() for p in baseline_model.parameters()):,}")

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(baseline_model.parameters(), lr=baseline_params['learning_rate'])

# Train baseline model
start_time = time.time()
baseline_history = train_model(baseline_model, train_loader, val_loader, criterion,
                               optimizer, epochs=baseline_params['epochs'])
baseline_time = time.time() - start_time

baseline_accuracy = evaluate_model(baseline_model, val_loader)
print(f"\nBaseline Model - Final Validation Accuracy: {baseline_accuracy:.2f}%")
print(f"Training Time: {baseline_time:.2f} seconds")

# Metaheuristic optimization helper functions
def create_model_with_params(params, vocab_size, max_length):
    """Create a model with given hyperparameters"""
    model = TextTransformer(
        vocab_size=vocab_size,
        embedding_dim=params['embedding_dim'],
        num_heads=params['num_heads'],
        num_layers=params['num_layers'],
        dim_feedforward=params['dim_feedforward'],
        num_classes=4,
        max_length=max_length,
        dropout=params['dropout']
    ).to(device)
    return model

def evaluate_hyperparameters(params, X_train, y_train, X_val, y_val,
                             vocab_size, max_length, epochs=5, verbose=False):
    """
    Evaluate a set of hyperparameters by training a model
    Returns validation accuracy (fitness score)
    """
    # Create data loaders
    train_dataset = TensorDataset(
        torch.LongTensor(X_train),
        torch.LongTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.LongTensor(X_val),
        torch.LongTensor(y_val)
    )

    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])

    # Create and train model
    model = create_model_with_params(params, vocab_size, max_length)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])

    # Train for fewer epochs in optimization
    history = train_model(model, train_loader, val_loader, criterion,
                         optimizer, epochs=epochs, verbose=verbose)

    # Return validation accuracy as fitness
    val_accuracy = history['val_acc'][-1]
    return val_accuracy, model

def random_hyperparameters():
    """Generate random hyperparameters within valid ranges"""
    # Ensure num_heads divides embedding_dim evenly
    embedding_dims = [64, 128, 256, 512]
    num_heads_options = {64: [2, 4], 128: [2, 4, 8], 256: [4, 8, 16], 512: [4, 8, 16]}

    embedding_dim = random.choice(embedding_dims)
    num_heads = random.choice(num_heads_options[embedding_dim])

    return {
        'embedding_dim': embedding_dim,
        'num_heads': num_heads,
        'num_layers': random.choice([1, 2, 3, 4]),
        'dim_feedforward': random.choice([256, 512, 1024, 2048]),
        'dropout': round(random.uniform(0.1, 0.5), 2),
        'learning_rate': random.choice([0.0001, 0.0005, 0.001, 0.005]),
        'batch_size': random.choice([32, 64, 128]),
        'epochs': 5  # Fixed for optimization
    }

# Particle Swarm Optimization (PSO)
class ParticleSwarmOptimization:
    def __init__(self, n_particles=5, n_iterations=5, w=0.7, c1=1.5, c2=1.5):
        self.n_particles = n_particles
        self.n_iterations = n_iterations
        self.w = w  # Inertia weight
        self.c1 = c1  # Cognitive parameter
        self.c2 = c2  # Social parameter

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("PARTICLE SWARM OPTIMIZATION")
        print("="*60)

        # Initialize particles (hyperparameter sets)
        particles = [random_hyperparameters() for _ in range(self.n_particles)]
        velocities = [{} for _ in range(self.n_particles)]

        # Evaluate initial particles
        fitness_scores = []
        for i, particle in enumerate(particles):
            print(f"\nEvaluating Particle {i+1}/{self.n_particles}")
            fitness, _ = evaluate_hyperparameters(
                particle, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
            )
            fitness_scores.append(fitness)
            print(f"Particle {i+1} Fitness: {fitness:.2f}%")

        # Personal best
        p_best = particles.copy()
        p_best_fitness = fitness_scores.copy()

        # Global best
        g_best_idx = np.argmax(p_best_fitness)
        g_best = p_best[g_best_idx].copy()
        g_best_fitness = p_best_fitness[g_best_idx]

        print(f"\nInitial Best Fitness: {g_best_fitness:.2f}%")

        # PSO iterations
        for iteration in range(self.n_iterations):
            print(f"\n--- PSO Iteration {iteration+1}/{self.n_iterations} ---")

            for i in range(self.n_particles):
                # Update velocity and position (simplified for discrete hyperparameters)
                # With some probability, move towards personal or global best
                if random.random() < 0.5:
                    # Move towards personal best
                    for key in p_best[i].keys():
                        if random.random() < self.c1 / (self.c1 + self.c2):
                            particles[i][key] = p_best[i][key]
                else:
                    # Move towards global best
                    for key in g_best.keys():
                        if random.random() < self.c2 / (self.c1 + self.c2):
                            particles[i][key] = g_best[key]

                # Add some randomness (exploration)
                if random.random() < 0.2:
                    particles[i] = random_hyperparameters()

                # Evaluate new position
                fitness, _ = evaluate_hyperparameters(
                    particles[i], X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
                )

                # Update personal best
                if fitness > p_best_fitness[i]:
                    p_best[i] = particles[i].copy()
                    p_best_fitness[i] = fitness
                    print(f"Particle {i+1}: New Personal Best = {fitness:.2f}%")

                # Update global best
                if fitness > g_best_fitness:
                    g_best = particles[i].copy()
                    g_best_fitness = fitness
                    print(f"*** New Global Best = {g_best_fitness:.2f}% ***")

        print(f"\nPSO Final Best Fitness: {g_best_fitness:.2f}%")
        print("Best Hyperparameters:")
        for key, value in g_best.items():
            print(f"  {key}: {value}")

        return g_best, g_best_fitness

# Simulated Annealing
class SimulatedAnnealing:
    def __init__(self, initial_temp=100, cooling_rate=0.85, n_iterations=10):
        self.initial_temp = initial_temp
        self.cooling_rate = cooling_rate
        self.n_iterations = n_iterations

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("SIMULATED ANNEALING")
        print("="*60)

        # Initialize with random solution
        current_solution = random_hyperparameters()
        current_fitness, _ = evaluate_hyperparameters(
            current_solution, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
        )

        best_solution = current_solution.copy()
        best_fitness = current_fitness

        print(f"Initial Fitness: {current_fitness:.2f}%")

        temperature = self.initial_temp

        for iteration in range(self.n_iterations):
            print(f"\n--- SA Iteration {iteration+1}/{self.n_iterations} ---")
            print(f"Temperature: {temperature:.2f}")

            # Generate neighbor solution (perturb current solution)
            neighbor_solution = current_solution.copy()

            # Randomly modify 1-2 hyperparameters
            keys_to_modify = random.sample(list(neighbor_solution.keys()),
                                          random.randint(1, 2))

            temp_params = random_hyperparameters()
            for key in keys_to_modify:
                if key != 'epochs':
                    neighbor_solution[key] = temp_params[key]

            # Ensure num_heads divides embedding_dim
            if neighbor_solution['embedding_dim'] % neighbor_solution['num_heads'] != 0:
                neighbor_solution = random_hyperparameters()

            # Evaluate neighbor
            neighbor_fitness, _ = evaluate_hyperparameters(
                neighbor_solution, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
            )

            print(f"Current Fitness: {current_fitness:.2f}%")
            print(f"Neighbor Fitness: {neighbor_fitness:.2f}%")

            # Acceptance criterion
            delta = neighbor_fitness - current_fitness

            if delta > 0:
                # Accept better solution
                current_solution = neighbor_solution
                current_fitness = neighbor_fitness
                print("✓ Accepted (Better solution)")

                if current_fitness > best_fitness:
                    best_solution = current_solution.copy()
                    best_fitness = current_fitness
                    print(f"*** New Best Fitness = {best_fitness:.2f}% ***")
            else:
                # Accept worse solution with probability
                acceptance_prob = np.exp(delta / temperature)
                if random.random() < acceptance_prob:
                    current_solution = neighbor_solution
                    current_fitness = neighbor_fitness
                    print(f"✓ Accepted (Probability: {acceptance_prob:.4f})")
                else:
                    print("✗ Rejected")

            # Cool down
            temperature *= self.cooling_rate

        print(f"\nSA Final Best Fitness: {best_fitness:.2f}%")
        print("Best Hyperparameters:")
        for key, value in best_solution.items():
            print(f"  {key}: {value}")

        return best_solution, best_fitness

# Ant Colony Optimization (ACO)
class AntColonyOptimization:
    def __init__(self, n_ants=5, n_iterations=5, evaporation_rate=0.5, alpha=1.0, beta=2.0):
        self.n_ants = n_ants
        self.n_iterations = n_iterations
        self.evaporation_rate = evaporation_rate
        self.alpha = alpha  # Pheromone importance
        self.beta = beta    # Heuristic importance

        # Define discrete choices for each hyperparameter
        self.param_choices = {
            'embedding_dim': [64, 128, 256, 512],
            'num_layers': [1, 2, 3, 4],
            'dim_feedforward': [256, 512, 1024, 2048],
            'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005],
            'batch_size': [32, 64, 128]
        }

        # Head choices depend on embedding dim
        self.head_choices = {
            64: [2, 4],
            128: [2, 4, 8],
            256: [4, 8, 16],
            512: [4, 8, 16]
        }

        # Initialize pheromone trails
        self.pheromones = {}
        for param, choices in self.param_choices.items():
            self.pheromones[param] = {choice: 1.0 for choice in map(str, choices)}

    def select_parameter(self, param):
        """Select parameter value based on pheromone trails"""
        choices = self.param_choices[param]
        pheromones = [self.pheromones[param][str(choice)] ** self.alpha for choice in choices]

        # Normalize to probabilities
        total = sum(pheromones)
        probabilities = [p / total for p in pheromones]

        # Select based on probability
        selected = np.random.choice(len(choices), p=probabilities)
        return choices[selected]

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("ANT COLONY OPTIMIZATION")
        print("="*60)

        best_solution = None
        best_fitness = 0

        for iteration in range(self.n_iterations):
            print(f"\n--- ACO Iteration {iteration+1}/{self.n_iterations} ---")

            # Each ant constructs a solution
            solutions = []
            fitness_scores = []

            for ant in range(self.n_ants):
                # Construct solution
                embedding_dim = self.select_parameter('embedding_dim')
                num_heads = random.choice(self.head_choices[embedding_dim])

                solution = {
                    'embedding_dim': embedding_dim,
                    'num_heads': num_heads,
                    'num_layers': self.select_parameter('num_layers'),
                    'dim_feedforward': self.select_parameter('dim_feedforward'),
                    'dropout': self.select_parameter('dropout'),
                    'learning_rate': self.select_parameter('learning_rate'),
                    'batch_size': self.select_parameter('batch_size'),
                    'epochs': 5
                }

                print(f"\nAnt {ant+1}/{self.n_ants}")

                # Evaluate solution
                fitness, _ = evaluate_hyperparameters(
                    solution, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
                )

                print(f"Ant {ant+1} Fitness: {fitness:.2f}%")

                solutions.append(solution)
                fitness_scores.append(fitness)

                # Update best solution
                if fitness > best_fitness:
                    best_solution = solution.copy()
                    best_fitness = fitness
                    print(f"*** New Best Fitness = {best_fitness:.2f}% ***")

            # Evaporate pheromones
            for param in self.pheromones:
                for choice in self.pheromones[param]:
                    self.pheromones[param][choice] *= (1 - self.evaporation_rate)

            # Deposit pheromones based on solution quality
            for solution, fitness in zip(solutions, fitness_scores):
                pheromone_deposit = fitness / 100.0  # Normalize

                for param, value in solution.items():
                    if param != 'epochs' and param != 'num_heads' and param in self.pheromones:
                        self.pheromones[param][str(value)] += pheromone_deposit

        print(f"\nACO Final Best Fitness: {best_fitness:.2f}%")
        print("Best Hyperparameters:")
        for key, value in best_solution.items():
            print(f"  {key}: {value}")

        return best_solution, best_fitness

# Tabu Search
class TabuSearch:
    def __init__(self, tabu_tenure=3, n_iterations=10, neighborhood_size=5):
        self.tabu_tenure = tabu_tenure
        self.n_iterations = n_iterations
        self.neighborhood_size = neighborhood_size

    def generate_neighborhood(self, solution):
        """Generate neighboring solutions"""
        neighbors = []

        for _ in range(self.neighborhood_size):
            neighbor = solution.copy()

            # Randomly modify 1 hyperparameter
            param_to_modify = random.choice([
                'embedding_dim', 'num_heads', 'num_layers', 'dim_feedforward',
                'dropout', 'learning_rate', 'batch_size'
            ])

            temp_params = random_hyperparameters()

            if param_to_modify == 'embedding_dim':
                # If changing embedding_dim, also update num_heads to be compatible
                neighbor['embedding_dim'] = temp_params['embedding_dim']
                neighbor['num_heads'] = temp_params['num_heads']
            else:
                neighbor[param_to_modify] = temp_params[param_to_modify]

            # Ensure compatibility
            if neighbor['embedding_dim'] % neighbor['num_heads'] != 0:
                neighbor = random_hyperparameters()

            neighbors.append(neighbor)

        return neighbors

    def solution_hash(self, solution):
        """Create a hashable representation of solution"""
        return str(sorted(solution.items()))

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("TABU SEARCH")
        print("="*60)

        # Initialize
        current_solution = random_hyperparameters()
        current_fitness, _ = evaluate_hyperparameters(
            current_solution, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
        )

        best_solution = current_solution.copy()
        best_fitness = current_fitness

        tabu_list = []

        print(f"Initial Fitness: {current_fitness:.2f}%")

        for iteration in range(self.n_iterations):
            print(f"\n--- Tabu Search Iteration {iteration+1}/{self.n_iterations} ---")

            # Generate neighborhood
            neighbors = self.generate_neighborhood(current_solution)

            # Evaluate neighbors
            best_neighbor = None
            best_neighbor_fitness = -float('inf')

            for i, neighbor in enumerate(neighbors):
                neighbor_hash = self.solution_hash(neighbor)

                # Skip if in tabu list
                if neighbor_hash in tabu_list:
                    continue

                print(f"Evaluating Neighbor {i+1}/{len(neighbors)}")

                fitness, _ = evaluate_hyperparameters(
                    neighbor, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
                )

                print(f"Neighbor {i+1} Fitness: {fitness:.2f}%")

                if fitness > best_neighbor_fitness:
                    best_neighbor = neighbor
                    best_neighbor_fitness = fitness

            # If we found a valid neighbor
            if best_neighbor is not None:
                current_solution = best_neighbor
                current_fitness = best_neighbor_fitness

                # Add to tabu list
                tabu_list.append(self.solution_hash(current_solution))

                # Maintain tabu list size
                if len(tabu_list) > self.tabu_tenure:
                    tabu_list.pop(0)

                print(f"Current Fitness: {current_fitness:.2f}%")

                # Update best solution
                if current_fitness > best_fitness:
                    best_solution = current_solution.copy()
                    best_fitness = current_fitness
                    print(f"*** New Best Fitness = {best_fitness:.2f}% ***")
            else:
                print("No valid neighbors found (all tabu)")

        print(f"\nTabu Search Final Best Fitness: {best_fitness:.2f}%")
        print("Best Hyperparameters:")
        for key, value in best_solution.items():
            print(f"  {key}: {value}")

        return best_solution, best_fitness

# Grey Wolf Optimizer
class GreyWolfOptimizer:
    def __init__(self, n_wolves=5, n_iterations=5):
        self.n_wolves = n_wolves
        self.n_iterations = n_iterations

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("GREY WOLF OPTIMIZER")
        print("="*60)

        # Initialize wolf population
        wolves = [random_hyperparameters() for _ in range(self.n_wolves)]

        # Evaluate initial population
        fitness_scores = []
        for i, wolf in enumerate(wolves):
            print(f"\nEvaluating Wolf {i+1}/{self.n_wolves}")
            fitness, _ = evaluate_hyperparameters(
                wolf, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
            )
            fitness_scores.append(fitness)
            print(f"Wolf {i+1} Fitness: {fitness:.2f}%")

        # Identify alpha, beta, and delta wolves
        sorted_indices = np.argsort(fitness_scores)[::-1]  # Descending order
        alpha_idx, beta_idx, delta_idx = sorted_indices[0], sorted_indices[1], sorted_indices[2]

        alpha_pos = wolves[alpha_idx].copy()
        beta_pos = wolves[beta_idx].copy()
        delta_pos = wolves[delta_idx].copy()

        alpha_score = fitness_scores[alpha_idx]
        beta_score = fitness_scores[beta_idx]
        delta_score = fitness_scores[delta_idx]

        print(f"\nInitial Hierarchy:")
        print(f"  Alpha (best): {alpha_score:.2f}%")
        print(f"  Beta (2nd): {beta_score:.2f}%")
        print(f"  Delta (3rd): {delta_score:.2f}%")

        # Define hyperparameter choices for position updates
        param_choices = {
            'embedding_dim': [64, 128, 256, 512],
            'num_layers': [1, 2, 3, 4],
            'dim_feedforward': [256, 512, 1024, 2048],
            'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005],
            'batch_size': [32, 64, 128]
        }

        # GWO iterations
        for iteration in range(self.n_iterations):
            a = 2 * (1 - iteration / self.n_iterations)  # Linearly decreasing from 2 to 0
            print(f"\n--- GWO Iteration {iteration+1}/{self.n_iterations} (a={a:.3f}) ---")

            for i in range(self.n_wolves):
                # Update each wolf's position based on alpha, beta, delta
                new_wolf = wolves[i].copy()

                # Randomly select which hyperparameters to update (mimicking position update)
                for param in param_choices.keys():
                    if param == 'embedding_dim':
                        continue  # Skip for now, handle with num_heads

                    r = np.random.random()

                    if r < 0.33:  # Influenced by alpha
                        new_wolf[param] = alpha_pos[param]
                    elif r < 0.66:  # Influenced by beta
                        new_wolf[param] = beta_pos[param]
                    else:  # Influenced by delta
                        new_wolf[param] = delta_pos[param]

                    # Add exploration with probability based on 'a'
                    if np.random.random() < a / 2:
                        new_wolf[param] = random.choice(param_choices[param])

                # Handle embedding_dim and num_heads together
                embedding_dims = [64, 128, 256, 512]
                num_heads_options = {64: [2, 4], 128: [2, 4, 8], 256: [4, 8, 16], 512: [4, 8, 16]}

                r = np.random.random()
                if r < 0.33:
                    new_wolf['embedding_dim'] = alpha_pos['embedding_dim']
                    new_wolf['num_heads'] = alpha_pos['num_heads']
                elif r < 0.66:
                    new_wolf['embedding_dim'] = beta_pos['embedding_dim']
                    new_wolf['num_heads'] = beta_pos['num_heads']
                else:
                    new_wolf['embedding_dim'] = delta_pos['embedding_dim']
                    new_wolf['num_heads'] = delta_pos['num_heads']

                # Exploration
                if np.random.random() < a / 2:
                    new_emb = random.choice(embedding_dims)
                    new_wolf['embedding_dim'] = new_emb
                    new_wolf['num_heads'] = random.choice(num_heads_options[new_emb])

                # Evaluate new position
                fitness, _ = evaluate_hyperparameters(
                    new_wolf, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
                )

                wolves[i] = new_wolf
                fitness_scores[i] = fitness

                # Update alpha, beta, delta
                if fitness > alpha_score:
                    delta_pos, delta_score = beta_pos.copy(), beta_score
                    beta_pos, beta_score = alpha_pos.copy(), alpha_score
                    alpha_pos, alpha_score = new_wolf.copy(), fitness
                    print(f"Wolf {i+1}: New Alpha = {fitness:.2f}%")
                elif fitness > beta_score:
                    delta_pos, delta_score = beta_pos.copy(), beta_score
                    beta_pos, beta_score = new_wolf.copy(), fitness
                    print(f"Wolf {i+1}: New Beta = {fitness:.2f}%")
                elif fitness > delta_score:
                    delta_pos, delta_score = new_wolf.copy(), fitness
                    print(f"Wolf {i+1}: New Delta = {fitness:.2f}%")

        print(f"\nGWO Final Best Fitness: {alpha_score:.2f}%")
        print("Best Hyperparameters:")
        for key, value in alpha_pos.items():
            print(f"  {key}: {value}")

        return alpha_pos, alpha_score

# Whale Optimization Algorithm
class WhaleOptimizationAlgorithm:
    def __init__(self, n_whales=5, n_iterations=5):
        self.n_whales = n_whales
        self.n_iterations = n_iterations

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*60)
        print("WHALE OPTIMIZATION ALGORITHM")
        print("="*60)

        # Initialize whale population
        whales = [random_hyperparameters() for _ in range(self.n_whales)]

        # Evaluate initial population
        fitness_scores = []
        for i, whale in enumerate(whales):
            print(f"\nEvaluating Whale {i+1}/{self.n_whales}")
            fitness, _ = evaluate_hyperparameters(
                whale, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
            )
            fitness_scores.append(fitness)
            print(f"Whale {i+1} Fitness: {fitness:.2f}%")

        # Find best whale (leader)
        best_idx = np.argmax(fitness_scores)
        best_whale = whales[best_idx].copy()
        best_score = fitness_scores[best_idx]

        print(f"\nInitial Best Fitness: {best_score:.2f}%")

        # Define hyperparameter choices
        param_choices = {
            'embedding_dim': [64, 128, 256, 512],
            'num_layers': [1, 2, 3, 4],
            'dim_feedforward': [256, 512, 1024, 2048],
            'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005],
            'batch_size': [32, 64, 128]
        }

        embedding_dims = [64, 128, 256, 512]
        num_heads_options = {64: [2, 4], 128: [2, 4, 8], 256: [4, 8, 16], 512: [4, 8, 16]}

        # WOA iterations
        for iteration in range(self.n_iterations):
            a = 2 * (1 - iteration / self.n_iterations)  # Linearly decreasing from 2 to 0
            a2 = -1 + iteration * (-1 / self.n_iterations)  # Linearly decreasing from -1 to -2

            print(f"\n--- WOA Iteration {iteration+1}/{self.n_iterations} (a={a:.3f}) ---")

            for i in range(self.n_whales):
                r = np.random.random()
                A = 2 * a * r - a
                C = 2 * r

                p = np.random.random()
                l = np.random.uniform(-1, 1)

                new_whale = whales[i].copy()

                if p < 0.5:  # Encircling prey or searching for prey
                    if abs(A) < 1:  # Encircling prey
                        # Move towards best whale
                        for param in param_choices.keys():
                            if np.random.random() < abs(C) / 2:
                                new_whale[param] = best_whale[param]
                            else:
                                # Random walk
                                new_whale[param] = random.choice(param_choices[param])

                        # Handle embedding_dim and num_heads
                        if np.random.random() < abs(C) / 2:
                            new_whale['embedding_dim'] = best_whale['embedding_dim']
                            new_whale['num_heads'] = best_whale['num_heads']
                        else:
                            new_emb = random.choice(embedding_dims)
                            new_whale['embedding_dim'] = new_emb
                            new_whale['num_heads'] = random.choice(num_heads_options[new_emb])

                    else:  # Search for prey (exploration)
                        # Select random whale
                        random_whale = whales[np.random.randint(0, self.n_whales)]

                        for param in param_choices.keys():
                            if np.random.random() < 0.5:
                                new_whale[param] = random_whale[param]
                            else:
                                new_whale[param] = random.choice(param_choices[param])

                        # Handle embedding_dim and num_heads
                        if np.random.random() < 0.5:
                            new_whale['embedding_dim'] = random_whale['embedding_dim']
                            new_whale['num_heads'] = random_whale['num_heads']
                        else:
                            new_emb = random.choice(embedding_dims)
                            new_whale['embedding_dim'] = new_emb
                            new_whale['num_heads'] = random.choice(num_heads_options[new_emb])

                else:  # Spiral updating position
                    # Move in spiral towards best whale
                    for param in param_choices.keys():
                        if np.random.random() < 0.7:  # High probability to follow best
                            new_whale[param] = best_whale[param]
                        else:
                            new_whale[param] = random.choice(param_choices[param])

                    # Handle embedding_dim and num_heads
                    if np.random.random() < 0.7:
                        new_whale['embedding_dim'] = best_whale['embedding_dim']
                        new_whale['num_heads'] = best_whale['num_heads']
                    else:
                        new_emb = random.choice(embedding_dims)
                        new_whale['embedding_dim'] = new_emb
                        new_whale['num_heads'] = random.choice(num_heads_options[new_emb])

                # Evaluate new position
                fitness, _ = evaluate_hyperparameters(
                    new_whale, X_train, y_train, X_val, y_val, vocab_size, max_length, epochs=5
                )

                whales[i] = new_whale
                fitness_scores[i] = fitness

                print(f"Whale {i+1} Fitness: {fitness:.2f}%")

                # Update best whale
                if fitness > best_score:
                    best_whale = new_whale.copy()
                    best_score = fitness
                    print(f"*** New Best Fitness = {best_score:.2f}% ***")

        print(f"\nWOA Final Best Fitness: {best_score:.2f}%")
        print("Best Hyperparameters:")
        for key, value in best_whale.items():
            print(f"  {key}: {value}")

        return best_whale, best_score

# Store results
results = {
    'Baseline': {
        'accuracy': baseline_accuracy,
        'time': baseline_time,
        'params': baseline_params
    }
}

# Run Tabu Search
start_time = time.time()
ts = TabuSearch(tabu_tenure=3, n_iterations=8, neighborhood_size=4)
ts_best_params, ts_fitness = ts.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
ts_time = time.time() - start_time

results['Tabu Search'] = {
    'accuracy': ts_fitness,
    'time': ts_time,
    'params': ts_best_params
}

# Run PSO
start_time = time.time()
pso = ParticleSwarmOptimization(n_particles=5, n_iterations=3)
pso_best_params, pso_fitness = pso.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
pso_time = time.time() - start_time

results['PSO'] = {
    'accuracy': pso_fitness,
    'time': pso_time,
    'params': pso_best_params
}

# Run Simulated Annealing
start_time = time.time()
sa = SimulatedAnnealing(initial_temp=100, cooling_rate=0.85, n_iterations=8)
sa_best_params, sa_fitness = sa.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
sa_time = time.time() - start_time

results['Simulated Annealing'] = {
    'accuracy': sa_fitness,
    'time': sa_time,
    'params': sa_best_params
}

# Run ACO
start_time = time.time()
aco = AntColonyOptimization(n_ants=4, n_iterations=3)
aco_best_params, aco_fitness = aco.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
aco_time = time.time() - start_time

results['ACO'] = {
    'accuracy': aco_fitness,
    'time': aco_time,
    'params': aco_best_params
}

# Run Grey Wolf Optimizer
start_time = time.time()
gwo = GreyWolfOptimizer(n_wolves=5, n_iterations=3)
gwo_best_params, gwo_fitness = gwo.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
gwo_time = time.time() - start_time

results['Grey Wolf'] = {
    'accuracy': gwo_fitness,
    'time': gwo_time,
    'params': gwo_best_params
}

# Run Whale Optimization Algorithm
start_time = time.time()
woa = WhaleOptimizationAlgorithm(n_whales=5, n_iterations=3)
woa_best_params, woa_fitness = woa.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)
woa_time = time.time() - start_time

results['Whale'] = {
    'accuracy': woa_fitness,
    'time': woa_time,
    'params': woa_best_params
}

# Train final models with optimized hyperparameters

final_results = {}

# Train each optimized model with full epochs
for method, data in results.items():
    if method == 'Baseline':
        final_results[method] = {
            'accuracy': data['accuracy'],
            'time': data['time']
        }
        continue

    print(f"\n\nTraining {method} Optimized Model...")
    print("="*50)

    params = data['params'].copy()
    params['epochs'] = 10  # Train for full epochs

    # Create data loaders
    train_loader = DataLoader(
        TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train)),
        batch_size=params['batch_size'],
        shuffle=True
    )
    val_loader = DataLoader(
        TensorDataset(torch.LongTensor(X_val), torch.LongTensor(y_val)),
        batch_size=params['batch_size']
    )

    # Create and train model
    model = create_model_with_params(params, len(preprocessor.word2idx), preprocessor.max_length)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])

    start_time = time.time()
    history = train_model(model, train_loader, val_loader, criterion,
                         optimizer, epochs=params['epochs'], verbose=True)
    training_time = time.time() - start_time

    final_accuracy = history['val_acc'][-1]

    final_results[method] = {
        'accuracy': final_accuracy,
        'time': training_time
    }

    print(f"\n{method} Final Accuracy: {final_accuracy:.2f}%")
    print(f"Training Time: {training_time:.2f} seconds")

"""# Results comparison and visualization

"""

# Create results DataFrame
comparison_df = pd.DataFrame({
    'Method': list(final_results.keys()),
    'Validation Accuracy (%)': [final_results[m]['accuracy'] for m in final_results.keys()],
    'Training Time (s)': [final_results[m]['time'] for m in final_results.keys()]
})

comparison_df = comparison_df.sort_values('Validation Accuracy (%)', ascending=False)
print("\n", comparison_df.to_string(index=False))

# Calculate improvements over baseline
baseline_acc = final_results['Baseline']['accuracy']
print(f"\n\nAccuracy Improvements Over Baseline ({baseline_acc:.2f}%):")
print("-" * 60)

for method in final_results.keys():
    if method != 'Baseline':
        improvement = final_results[method]['accuracy'] - baseline_acc
        print(f"{method:25s}: {improvement:+.2f}%")

# Visualization - Accuracy Comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 5))

# Accuracy comparison
methods = list(final_results.keys())
accuracies = [final_results[m]['accuracy'] for m in methods]

# Updated color palette for 7 methods
colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#9B59B6', '#1ABC9C']
bars = axes[0].bar(methods, accuracies, color=colors[:len(methods)], alpha=0.8, edgecolor='black')

axes[0].set_ylabel('Validation Accuracy (%)', fontsize=12, fontweight='bold')
axes[0].set_title('Transformer Model Performance Comparison', fontsize=14, fontweight='bold')
axes[0].set_ylim([min(accuracies) - 2, max(accuracies) + 2])
axes[0].grid(axis='y', alpha=0.3, linestyle='--')
axes[0].tick_params(axis='x', rotation=25)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    axes[0].text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%',
                ha='center', va='bottom', fontweight='bold', fontsize=9)

# Training time comparison
times = [final_results[m]['time'] for m in methods]
bars2 = axes[1].bar(methods, times, color=colors[:len(methods)], alpha=0.8, edgecolor='black')

axes[1].set_ylabel('Training Time (seconds)', fontsize=12, fontweight='bold')
axes[1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3, linestyle='--')
axes[1].tick_params(axis='x', rotation=25)

# Add value labels on bars
for bar in bars2:
    height = bar.get_height()
    axes[1].text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}s',
                ha='center', va='bottom', fontweight='bold', fontsize=9)

plt.tight_layout()
plt.savefig('transformer_metaheuristic_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

"""# Detailed hyperparameter comparison

"""

param_comparison = []
for method, data in results.items():
    params = data['params']
    param_comparison.append({
        'Method': method,
        'Embedding Dim': params.get('embedding_dim', 'N/A'),
        'Num Heads': params.get('num_heads', 'N/A'),
        'Num Layers': params.get('num_layers', 'N/A'),
        'Dim FFN': params.get('dim_feedforward', 'N/A'),
        'Dropout': params.get('dropout', 'N/A'),
        'Learning Rate': params.get('learning_rate', 'N/A'),
        'Batch Size': params.get('batch_size', 'N/A')
    })

param_df = pd.DataFrame(param_comparison)
print("\n", param_df.to_string(index=False))

"""# **Phase 2: Parameter & Explainability Optimization**"""

!pip install shap lime
import shap
from lime.lime_text import LimeTextExplainer

def evaluate_pso_with_params(w, c1, c2, X_train, y_train, X_val, y_val, vocab_size, max_length):
    """Evaluate PSO with specific parameters"""
    print(f"  Testing PSO: w={w:.2f}, c1={c1:.2f}, c2={c2:.2f}")
    pso = ParticleSwarmOptimization(n_particles=3, n_iterations=2, w=w, c1=c1, c2=c2)
    _, fitness = pso.optimize(X_train, y_train, X_val, y_val, vocab_size, max_length)
    return fitness

class HillClimbingPSOOptimizer:
    def __init__(self, max_iters=4):
        self.max_iters = max_iters

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*70)
        print("HILL CLIMBING: OPTIMIZING PSO PARAMETERS (w, c1, c2)")
        print("="*70)

        # Start with default
        w, c1, c2 = 0.7, 1.5, 1.5
        current_fitness = evaluate_pso_with_params(w, c1, c2, X_train, y_train, X_val, y_val, vocab_size, max_length)

        best_w, best_c1, best_c2 = w, c1, c2
        best_fitness = current_fitness

        print(f"Initial: w={w}, c1={c1}, c2={c2}, fitness={current_fitness:.2f}%")

        for it in range(self.max_iters):
            print(f"\n--- Iteration {it+1}/{self.max_iters} ---")

            improved = False
            # Try neighbors
            for dw in [-0.1, 0.1]:
                new_w = max(0.3, min(0.9, w + dw))
                fitness = evaluate_pso_with_params(new_w, c1, c2, X_train, y_train, X_val, y_val, vocab_size, max_length)
                if fitness > current_fitness:
                    w, current_fitness, improved = new_w, fitness, True
                    print(f"  ✓ Improved with w={w:.2f}")
                    break

            if not improved:
                for dc1 in [-0.3, 0.3]:
                    new_c1 = max(0.5, min(2.5, c1 + dc1))
                    fitness = evaluate_pso_with_params(w, new_c1, c2, X_train, y_train, X_val, y_val, vocab_size, max_length)
                    if fitness > current_fitness:
                        c1, current_fitness, improved = new_c1, fitness, True
                        print(f"  ✓ Improved with c1={c1:.2f}")
                        break

            if not improved:
                for dc2 in [-0.3, 0.3]:
                    new_c2 = max(0.5, min(2.5, c2 + dc2))
                    fitness = evaluate_pso_with_params(w, c1, new_c2, X_train, y_train, X_val, y_val, vocab_size, max_length)
                    if fitness > current_fitness:
                        c2, current_fitness, improved = new_c2, fitness, True
                        print(f"  ✓ Improved with c2={c2:.2f}")
                        break

            if not improved:
                print("  No improvement, stopping")
                break

            if current_fitness > best_fitness:
                best_w, best_c1, best_c2, best_fitness = w, c1, c2, current_fitness

        print(f"\n{'='*70}")
        print(f"OPTIMIZED PSO PARAMS: w={best_w:.2f}, c1={best_c1:.2f}, c2={best_c2:.2f}")
        print(f"Best Performance: {best_fitness:.2f}%")
        print(f"{'='*70}")

        return best_w, best_c1, best_c2, best_fitness

class FireflySAOptimizer:
    def __init__(self, n_fireflies=4, n_iters=3):
        self.n_fireflies = n_fireflies
        self.n_iters = n_iters

    def evaluate_sa(self, temp, cooling, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print(f"  Testing SA: temp={temp:.1f}, cooling={cooling:.3f}")
        sa = SimulatedAnnealing(initial_temp=temp, cooling_rate=cooling, n_iterations=4)
        _, fitness = sa.optimize(X_train, y_train, X_val, y_val, vocab_size, max_length)
        return fitness

    def optimize(self, X_train, y_train, X_val, y_val, vocab_size, max_length):
        print("\n" + "="*70)
        print("FIREFLY: OPTIMIZING SA PARAMETERS (temp, cooling_rate)")
        print("="*70)

        # Initialize fireflies
        temps = [random.uniform(50, 150) for _ in range(self.n_fireflies)]
        coolings = [random.uniform(0.75, 0.95) for _ in range(self.n_fireflies)]
        fitness = [self.evaluate_sa(temps[i], coolings[i], X_train, y_train, X_val, y_val, vocab_size, max_length)
                   for i in range(self.n_fireflies)]

        best_idx = np.argmax(fitness)
        best_temp, best_cooling, best_fitness = temps[best_idx], coolings[best_idx], fitness[best_idx]

        print(f"Initial best: temp={best_temp:.1f}, cooling={best_cooling:.3f}, fitness={best_fitness:.2f}%")

        for it in range(self.n_iters):
            print(f"\n--- Iteration {it+1}/{self.n_iters} ---")

            for i in range(self.n_fireflies):
                for j in range(self.n_fireflies):
                    if fitness[j] > fitness[i]:
                        # Move i towards j
                        if random.random() < 0.7:
                            temps[i] = (temps[i] + temps[j]) / 2
                            coolings[i] = (coolings[i] + coolings[j]) / 2

                        new_fitness = self.evaluate_sa(temps[i], coolings[i], X_train, y_train, X_val, y_val, vocab_size, max_length)

                        if new_fitness > fitness[i]:
                            fitness[i] = new_fitness
                            if new_fitness > best_fitness:
                                best_temp, best_cooling, best_fitness = temps[i], coolings[i], new_fitness
                                print(f"  ✓ New best: {best_fitness:.2f}%")

        print(f"\n{'='*70}")
        print(f"OPTIMIZED SA PARAMS: temp={best_temp:.1f}, cooling={best_cooling:.3f}")
        print(f"Best Performance: {best_fitness:.2f}%")
        print(f"{'='*70}")

        return best_temp, best_cooling, best_fitness

hc = HillClimbingPSOOptimizer(max_iters=3)
opt_w, opt_c1, opt_c2, pso_perf = hc.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)

ff = FireflySAOptimizer(n_fireflies=4, n_iters=2)
opt_temp, opt_cool, sa_perf = ff.optimize(
    X_train, y_train, X_val, y_val, len(preprocessor.word2idx), preprocessor.max_length
)

class LIMETextExplainer:
    def __init__(self, model, preprocessor, class_names):
        self.model = model
        self.preprocessor = preprocessor
        self.class_names = class_names
        self.model.eval()

    def predict_proba(self, texts):
        """Prediction function for LIME"""
        encoded = self.preprocessor.encode(texts)
        with torch.no_grad():
            inputs = torch.LongTensor(encoded).to(device)
            outputs = self.model(inputs)
            probs = torch.softmax(outputs, dim=1)
        return probs.cpu().numpy()

    def explain(self, text, num_features=10, num_samples=500):
        """Generate LIME explanation"""
        explainer = LimeTextExplainer(class_names=self.class_names)

        exp = explainer.explain_instance(
            text,
            self.predict_proba,
            num_features=num_features,
            num_samples=num_samples
        )

        return exp

    def get_stability_score(self, text, num_runs=3):
        """Measure explanation stability"""
        explanations = []
        for _ in range(num_runs):
            exp = self.explain(text, num_features=10, num_samples=500)
            explanations.append(dict(exp.as_list()))

        # Check consistency of top features
        all_features = set()
        for exp in explanations:
            all_features.update(exp.keys())

        # Calculate variance of feature importances
        variances = []
        for feature in all_features:
            values = [exp.get(feature, 0) for exp in explanations]
            variances.append(np.var(values))

        stability = 1.0 / (1.0 + np.mean(variances))
        return stability

"""# Prepare Model and Test Samples

"""

best_method = max(results.items(), key=lambda x: x[1]['accuracy'])[0]
best_params = results[best_method]['params'].copy()
best_params['epochs'] = 5

print(f"Using {best_method} model for XAI")

# Train model
train_loader = DataLoader(
    TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train)),
    batch_size=best_params['batch_size'], shuffle=True
)
val_loader = DataLoader(
    TensorDataset(torch.LongTensor(X_val), torch.LongTensor(y_val)),
    batch_size=best_params['batch_size']
)

xai_model = create_model_with_params(best_params, len(preprocessor.word2idx), preprocessor.max_length)
optimizer = optim.Adam(xai_model.parameters(), lr=best_params['learning_rate'])
criterion = nn.CrossEntropyLoss()

print("Training model for XAI...")
train_model(xai_model, train_loader, val_loader, criterion, optimizer, epochs=5, verbose=False)

# Select test samples
test_samples = val_texts[:5]
class_names = ['World', 'Sports', 'Business', 'Sci/Tech']

class PSO_LIME_Optimizer:
    def __init__(self, n_particles=3, n_iters=2):
        self.n_particles = n_particles
        self.n_iters = n_iters

    def evaluate(self, num_features, num_samples, model, preprocessor, test_texts):
        explainer = LIMETextExplainer(model, preprocessor, class_names)
        scores = []
        for text in test_texts[:2]:  # Use 2 samples for speed
            score = explainer.get_stability_score(text, num_runs=2)
            scores.append(score)
        return np.mean(scores)

    def optimize(self, model, preprocessor, test_texts):
        print("\n" + "="*60)
        print("PSO: OPTIMIZING LIME PARAMETERS")
        print("="*60)

        # Parameter choices
        feat_choices = [5, 10, 15, 20]
        samp_choices = [100, 300, 500, 1000]

        # Initialize particles
        particles = [{'num_features': random.choice(feat_choices),
                     'num_samples': random.choice(samp_choices)}
                    for _ in range(self.n_particles)]

        fitness = [self.evaluate(p['num_features'], p['num_samples'], model, preprocessor, test_texts)
                  for p in particles]

        p_best = particles.copy()
        p_best_fit = fitness.copy()

        g_best_idx = np.argmax(p_best_fit)
        g_best = p_best[g_best_idx].copy()
        g_best_fit = p_best_fit[g_best_idx]

        print(f"Initial best: {g_best}, score={g_best_fit:.4f}")

        for it in range(self.n_iters):
            print(f"\n--- Iteration {it+1}/{self.n_iters} ---")

            for i in range(self.n_particles):
                # Move towards global best
                if random.random() < 0.6:
                    particles[i] = g_best.copy()
                else:
                    particles[i] = {'num_features': random.choice(feat_choices),
                                   'num_samples': random.choice(samp_choices)}

                fit = self.evaluate(particles[i]['num_features'], particles[i]['num_samples'],
                                   model, preprocessor, test_texts)

                if fit > p_best_fit[i]:
                    p_best[i] = particles[i].copy()
                    p_best_fit[i] = fit

                if fit > g_best_fit:
                    g_best = particles[i].copy()
                    g_best_fit = fit
                    print(f"  ✓ New best: {g_best}, score={g_best_fit:.4f}")

        print(f"\n{'='*60}")
        print(f"OPTIMIZED LIME: {g_best}, score={g_best_fit:.4f}")
        print(f"{'='*60}")

        return g_best, g_best_fit

class SA_LIME_Optimizer:
    def __init__(self, temp=30, cooling=0.85, n_iters=5):
        self.temp = temp
        self.cooling = cooling
        self.n_iters = n_iters

    def evaluate(self, num_features, num_samples, model, preprocessor, test_texts):
        explainer = LIMETextExplainer(model, preprocessor, class_names)
        scores = []
        for text in test_texts[:2]:
            score = explainer.get_stability_score(text, num_runs=2)
            scores.append(score)
        return np.mean(scores)

    def get_neighbor(self, current):
        feat_choices = [5, 10, 15, 20]
        samp_choices = [100, 300, 500, 1000]

        neighbor = current.copy()
        if random.random() < 0.5:
            neighbor['num_features'] = random.choice(feat_choices)
        else:
            neighbor['num_samples'] = random.choice(samp_choices)
        return neighbor

    def optimize(self, model, preprocessor, test_texts):
        print("\n" + "="*60)
        print("SA: OPTIMIZING LIME PARAMETERS")
        print("="*60)

        current = {'num_features': 10, 'num_samples': 300}
        current_fit = self.evaluate(current['num_features'], current['num_samples'],
                                    model, preprocessor, test_texts)

        best = current.copy()
        best_fit = current_fit
        temp = self.temp

        print(f"Initial: {current}, score={current_fit:.4f}")

        for it in range(self.n_iters):
            print(f"\n--- Iteration {it+1}/{self.n_iters} (T={temp:.2f}) ---")

            neighbor = self.get_neighbor(current)
            neighbor_fit = self.evaluate(neighbor['num_features'], neighbor['num_samples'],
                                        model, preprocessor, test_texts)

            delta = neighbor_fit - current_fit

            if delta > 0 or random.random() < np.exp(delta / temp):
                current = neighbor
                current_fit = neighbor_fit
                print(f"  ✓ Accepted: {current}, score={current_fit:.4f}")

                if current_fit > best_fit:
                    best = current.copy()
                    best_fit = current_fit
                    print(f"  *** New best: {best_fit:.4f}")

            temp *= self.cooling

        print(f"\n{'='*60}")
        print(f"OPTIMIZED LIME: {best}, score={best_fit:.4f}")
        print(f"{'='*60}")

        return best, best_fit

class Tabu_LIME_Optimizer:
    def __init__(self, tenure=2, n_iters=5, n_neighbors=3):
        self.tenure = tenure
        self.n_iters = n_iters
        self.n_neighbors = n_neighbors

    def evaluate(self, num_features, num_samples, model, preprocessor, test_texts):
        explainer = LIMETextExplainer(model, preprocessor, class_names)
        scores = []
        for text in test_texts[:2]:
            score = explainer.get_stability_score(text, num_runs=2)
            scores.append(score)
        return np.mean(scores)

    def optimize(self, model, preprocessor, test_texts):
        print("\n" + "="*60)
        print("TABU: OPTIMIZING LIME PARAMETERS")
        print("="*60)

        feat_choices = [5, 10, 15, 20]
        samp_choices = [100, 300, 500, 1000]

        current = {'num_features': 10, 'num_samples': 300}
        current_fit = self.evaluate(current['num_features'], current['num_samples'],
                                    model, preprocessor, test_texts)

        best = current.copy()
        best_fit = current_fit
        tabu = []

        print(f"Initial: {current}, score={current_fit:.4f}")

        for it in range(self.n_iters):
            print(f"\n--- Iteration {it+1}/{self.n_iters} ---")

            neighbors = [{'num_features': random.choice(feat_choices),
                         'num_samples': random.choice(samp_choices)}
                        for _ in range(self.n_neighbors)]

            best_neighbor = None
            best_neighbor_fit = -1

            for n in neighbors:
                n_hash = f"{n['num_features']}_{n['num_samples']}"
                if n_hash in tabu:
                    continue

                fit = self.evaluate(n['num_features'], n['num_samples'],
                                   model, preprocessor, test_texts)

                if fit > best_neighbor_fit:
                    best_neighbor = n
                    best_neighbor_fit = fit

            if best_neighbor:
                current = best_neighbor
                current_fit = best_neighbor_fit
                tabu.append(f"{current['num_features']}_{current['num_samples']}")

                if len(tabu) > self.tenure:
                    tabu.pop(0)

                if current_fit > best_fit:
                    best = current.copy()
                    best_fit = current_fit
                    print(f"  ✓ New best: {best}, score={best_fit:.4f}")

        print(f"\n{'='*60}")
        print(f"OPTIMIZED LIME: {best}, score={best_fit:.4f}")
        print(f"{'='*60}")

        return best, best_fit

class ACO_LIME_Optimizer:
    def __init__(self, n_ants=3, n_iters=3, evap=0.5):
        self.n_ants = n_ants
        self.n_iters = n_iters
        self.evap = evap

        self.feat_choices = [5, 10, 15, 20]
        self.samp_choices = [100, 300, 500, 1000]

        self.pheromones = {
            'feat': {str(c): 1.0 for c in self.feat_choices},
            'samp': {str(c): 1.0 for c in self.samp_choices}
        }

    def evaluate(self, num_features, num_samples, model, preprocessor, test_texts):
        explainer = LIMETextExplainer(model, preprocessor, class_names)
        scores = []
        for text in test_texts[:2]:
            score = explainer.get_stability_score(text, num_runs=2)
            scores.append(score)
        return np.mean(scores)

    def select(self, param_name, choices):
        pheromones = [self.pheromones[param_name][str(c)] for c in choices]
        total = sum(pheromones)
        probs = [p/total for p in pheromones]
        return choices[np.random.choice(len(choices), p=probs)]

    def optimize(self, model, preprocessor, test_texts):
        print("\n" + "="*60)
        print("ACO: OPTIMIZING LIME PARAMETERS")
        print("="*60)

        best = None
        best_fit = 0

        for it in range(self.n_iters):
            print(f"\n--- Iteration {it+1}/{self.n_iters} ---")

            solutions = []
            fitness = []

            for ant in range(self.n_ants):
                sol = {
                    'num_features': self.select('feat', self.feat_choices),
                    'num_samples': self.select('samp', self.samp_choices)
                }

                fit = self.evaluate(sol['num_features'], sol['num_samples'],
                                   model, preprocessor, test_texts)

                solutions.append(sol)
                fitness.append(fit)

                print(f"  Ant {ant+1}: {sol}, score={fit:.4f}")

                if fit > best_fit:
                    best = sol.copy()
                    best_fit = fit
                    print(f"    ✓ New best!")

            # Update pheromones
            for p in self.pheromones:
                for c in self.pheromones[p]:
                    self.pheromones[p][c] *= (1 - self.evap)

            for sol, fit in zip(solutions, fitness):
                deposit = fit
                self.pheromones['feat'][str(sol['num_features'])] += deposit
                self.pheromones['samp'][str(sol['num_samples'])] += deposit

        print(f"\n{'='*60}")
        print(f"OPTIMIZED LIME: {best}, score={best_fit:.4f}")
        print(f"{'='*60}")

        return best, best_fit

xai_results = {}

"""# Run XAI Optimization"""

xai_results = {}

# PSO
pso_lime = PSO_LIME_Optimizer(n_particles=3, n_iters=2)
pso_params, pso_score = pso_lime.optimize(xai_model, preprocessor, test_samples)
xai_results['PSO'] = {'params': pso_params, 'score': pso_score}

# SA
sa_lime = SA_LIME_Optimizer(temp=20, cooling=0.85, n_iters=4)
sa_params, sa_score = sa_lime.optimize(xai_model, preprocessor, test_samples)
xai_results['SA'] = {'params': sa_params, 'score': sa_score}

# Tabu
tabu_lime = Tabu_LIME_Optimizer(tenure=2, n_iters=4, n_neighbors=3)
tabu_params, tabu_score = tabu_lime.optimize(xai_model, preprocessor, test_samples)
xai_results['Tabu'] = {'params': tabu_params, 'score': tabu_score}

# ACO
aco_lime = ACO_LIME_Optimizer(n_ants=3, n_iters=2, evap=0.5)
aco_params, aco_score = aco_lime.optimize(xai_model, preprocessor, test_samples)
xai_results['ACO'] = {'params': aco_params, 'score': aco_score}

"""# Final Results"""

print("\n1. ALGORITHM PARAMETER OPTIMIZATION:")
print(f"   PSO optimized (Hill Climbing): w={opt_w:.2f}, c1={opt_c1:.2f}, c2={opt_c2:.2f}")
print(f"   Performance: {pso_perf:.2f}%")
print(f"\n   SA optimized (Firefly): temp={opt_temp:.1f}, cooling={opt_cool:.3f}")
print(f"   Performance: {sa_perf:.2f}%")

print("\n2. XAI OPTIMIZATION (LIME parameters):")
for method, data in xai_results.items():
    print(f"   {method}: {data['params']}, stability={data['score']:.4f}")

# Best XAI method
best_xai = max(xai_results.items(), key=lambda x: x[1]['score'])
print(f"\n   Best XAI Method: {best_xai[0]} with score {best_xai[1]['score']:.4f}")

best_xai_params = best_xai[1]['params']
final_explainer = LIMETextExplainer(xai_model, preprocessor, class_names)

print(f"\nUsing optimized LIME parameters: {best_xai_params}")
print("\nExplanations for sample texts:")

for i, text in enumerate(test_samples[:3]):
    print(f"\n--- Sample {i+1} ---")
    print(f"Text: {text[:100]}...")

    exp = final_explainer.explain(
        text,
        num_features=best_xai_params['num_features'],
        num_samples=best_xai_params['num_samples']
    )

    print(f"Predicted class: {class_names[exp.top_labels[0]]}")
    print("Top features:")
    for feature, weight in exp.as_list()[:5]:
        print(f"  '{feature}': {weight:.3f}")

"""# Summary"""

print(f"\nDataset: AG News (20,000 samples, 4 classes)")
print(f"Baseline Accuracy: {baseline_accuracy:.2f}%")
print(f"\nBest Model: {best_method}")
print(f"Best Accuracy: {max(final_results.values(), key=lambda x: x['accuracy'])['accuracy']:.2f}%")
print(f"Improvement: +{max(final_results.values(), key=lambda x: x['accuracy'])['accuracy'] - baseline_accuracy:.2f}%")
print(f"\nTotal Unique Algorithms Used: 8")
print(f"- Phase 1: PSO, SA, ACO, Tabu, GWO, WOA")
print(f"- Phase 2: Hill Climbing, Firefly")
print(f"- XAI: PSO, SA, Tabu, ACO (reused)")